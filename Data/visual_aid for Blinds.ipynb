{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "from time import time\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "from keras.layers.merge import add\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting caption data from json file\n",
    "\n",
    "def collectCaption(path):\n",
    "    with open(path) as f:\n",
    "        captions = json.load(f)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = collectCaption(\"./Dataset/annotations/captions_train2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'images', 'annotations'])\n"
     ]
    }
   ],
   "source": [
    "print(captions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping ids to its images.\n",
    "id_img = {}\n",
    "for x in captions['images']:\n",
    "#     c +=1\n",
    "    id_img[str(x['id'])] = x['file_name']\n",
    "#     if c==10:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000391895.jpg\n"
     ]
    }
   ],
   "source": [
    "print(id_img[\"391895\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = {}\n",
    "for anno in captions['annotations']:\n",
    "    img_id = str(anno['image_id'])\n",
    "    cap = anno['caption']\n",
    "    \n",
    "    img_name = id_img[img_id]\n",
    "    if description.get(img_name) is None:\n",
    "        description[img_name] = []\n",
    "    if len(description[img_name]) <= 5:\n",
    "        description[img_name].append(cap)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A woman wearing a net on her head cutting a cake. ',\n",
       " 'A woman cutting a large white sheet cake.',\n",
       " 'A woman wearing a hair net cutting a large sheet cake.',\n",
       " 'there is a woman that is cutting a white cake',\n",
       " \"A woman marking a cake with the back of a chef's knife. \"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description['000000522418.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(\"[^a-z]+\",\" \",sent)\n",
    "    sent = sent.split()\n",
    "    \n",
    "    sent = [s for s in sent if len(s)>1]\n",
    "    sent = \" \".join(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning description\n",
    "\n",
    "for key,caption_list in description.items():\n",
    "    for i in range(len(caption_list)):\n",
    "        caption_list[i] = clean_text(caption_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman wearing net on her head cutting cake',\n",
       " 'woman cutting large white sheet cake',\n",
       " 'woman wearing hair net cutting large sheet cake',\n",
       " 'there is woman that is cutting white cake',\n",
       " 'woman marking cake with the back of chef knife']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description['000000522418.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"discription.txt\",\"w\") as f:\n",
    "#     f.write(str(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocab\n",
    "description = None\n",
    "with open(\"Data\\discription.txt\",\"r\") as f:\n",
    "    description = f.read()\n",
    "json_acceptable_string = description.replace(\"'\",\"\\\"\")\n",
    "description = json.loads(json_acceptable_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman wearing net on her head cutting cake',\n",
       " 'woman cutting large white sheet cake',\n",
       " 'woman wearing hair net cutting large sheet cake',\n",
       " 'there is woman that is cutting white cake',\n",
       " 'woman marking cake with the back of chef knife']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description['000000522418.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26440\n"
     ]
    }
   ],
   "source": [
    "# vocab\n",
    "\n",
    "vocab = set()\n",
    "for key in description.keys():\n",
    "    [vocab.update(sent.split()) for sent in description[key]]\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5210675\n"
     ]
    }
   ],
   "source": [
    "# total no. of words accross the descriptionabs\n",
    "total_words = []\n",
    "\n",
    "for key in description.keys():\n",
    "    [total_words.append(i) for des in description[key] for i in des.split()]\n",
    "print(len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26440\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter(total_words)\n",
    "freq_cnt = dict(counter)\n",
    "print(len(freq_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq_cnt = sorted(freq_cnt.items(),reverse=True, key=lambda x:x[1])\n",
    "\n",
    "#filter\n",
    "threshold = 4\n",
    "sorted_freq_cnt = [x for x in sorted_freq_cnt if x[1]>threshold]\n",
    "total_words = [x[0] for x in sorted_freq_cnt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n"
     ]
    }
   ],
   "source": [
    "print(len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train images\n",
    "train_img_id = []\n",
    "for key,img in id_img.items():\n",
    "    train_img_id.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000000391895.jpg', '000000522418.jpg', '000000184613.jpg', '000000318219.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(train_img_id[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118287"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Description for the Training Data\n",
    "# Tweak - Add <s> and <e> toen to our traing data\n",
    "\n",
    "train_descriptions = {}\n",
    "\n",
    "for img_id in train_img_id:\n",
    "    train_descriptions[img_id] = []\n",
    "    for cap in description[img_id]:\n",
    "        cap_to_append = \"<s> \" + cap + \" <e>\"\n",
    "        train_descriptions[img_id].append(cap_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> man with red helmet on small moped on dirt road <e>',\n",
       " '<s> man riding motor bike on dirt road on the countryside <e>',\n",
       " '<s> man riding on the back of motorcycle <e>',\n",
       " '<s> dirt path with young person on motor bike rests to the foreground of verdant area with bridge and background of cloud wreathed mountains <e>',\n",
       " '<s> man in red shirt and red hat is on motorcycle on hill side <e>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions[\"000000391895.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# = \"D:/programming/Machine learning and Deep learning/Projects/minor1.0/videoCaptioning for blinds/Datasets/coco/train2017/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning\n",
    "# - images-->Features\n",
    "\n",
    "#Step-1 Download Pre-trained model--resnet-50\n",
    "\n",
    "# model = ResNet50(weights='imagenet',input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_img(img):\n",
    "#     img = image.load_img(img,target_size=(224,224))\n",
    "#     img = image.img_to_array(img)\n",
    "#     img = np.expand_dims(img,axis=0)\n",
    "    \n",
    "#     #Normalization\n",
    "    \n",
    "#     img = preprocess_input(img)\n",
    "#     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_image(img):\n",
    "#     img = preprocess_img(img)\n",
    "#     feature_vect = model_new.predict(img)\n",
    "#     feature_vect = feature_vect.reshape((-1,))\n",
    "#     return feature_vect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_image(IMG_PATH+ \"000000522418.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding_train = {}\n",
    "# t0 = time()\n",
    "# for ix,img_id in enumerate(train_img_id):\n",
    "#     img_path = IMG_PATH+img_id\n",
    "#     encoding_train[img_id] = encode_image(img_path)\n",
    "    \n",
    "#     if ix%1000==0:\n",
    "#         print(\"Encoding in progress time step %d \"%ix)\n",
    "# end_t = time()\n",
    "# print(\"total time taken :\",end_t-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store this on disk\n",
    "\n",
    "# with open(\"encoded_train.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(encoding_train,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load img feature vectors in ram from disk\n",
    "\n",
    "with open(\"./Data/encoded_train.pkl\",\"rb\") as f:\n",
    "       encoding_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44277993 0.23587297 0.35735822 ... 5.404309   0.3640494  1.1392363 ]\n"
     ]
    }
   ],
   "source": [
    "print((encoding_train['000000522418.jpg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  preparing caption dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_idx = {}\n",
    "idx_2_word = {}\n",
    "\n",
    "for i,word in enumerate(total_words):\n",
    "    word_2_idx[word] = i+1\n",
    "    idx_2_word[i+1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'women'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_idx['women']\n",
    "idx_2_word[185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n"
     ]
    }
   ],
   "source": [
    "print(len(idx_2_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_idx['<s>'] = 10101\n",
    "idx_2_word[10101] = \"<s>\"\n",
    "\n",
    "word_2_idx['<e>'] = 10102\n",
    "idx_2_word[10102] = \"<e>\"\n",
    "\n",
    "vocab_size = len(idx_2_word) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10103\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for key in train_descriptions.keys():\n",
    "    for cap in train_descriptions[key]:\n",
    "        max_len = max(max_len,len(cap.split()))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader(Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_descriptions,encoding_train,word_2_idx,max_len,batch_size):\n",
    "    X1,X2,y = [],[],[]\n",
    "    \n",
    "    n=0\n",
    "    while True:\n",
    "        for key,desc_list in train_descriptions.items():\n",
    "            n += 1\n",
    "            photo = encoding_train[key]\n",
    "            for desc in desc_list:\n",
    "                seq = [word_2_idx[word] for word in desc.split() if word in word_2_idx]\n",
    "                \n",
    "                for i in range(1,len(seq)):\n",
    "                    xi = seq[0:i]\n",
    "                    yi = seq[i]\n",
    "                    \n",
    "                    xi = pad_sequences([xi], maxlen=max_len, value=0, padding='post')[0]\n",
    "                    yi = to_categorical([yi], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    X1.append(photo)\n",
    "                    X2.append(xi)\n",
    "                    y.append(yi)\n",
    "                    \n",
    "            if n==batch_size:\n",
    "                yield [[np.array(X1),np.array(X2)],np.array(y)]\n",
    "\n",
    "                X1,X2,y = [],[],[]\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec enbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"D:/programming/Machine learning and Deep learning/Projects/minor1.0/videoCaptioning for blinds/Datasets/glove.6B.50d.txt\",encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    \n",
    "    word_embeddings = np.array(values[1:], dtype='float')\n",
    "    embedding_index[word] = word_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix():\n",
    "    emb_dim = 50\n",
    "    matrix = np.zeros((vocab_size,emb_dim))\n",
    "    for word,idx in word_2_idx.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            matrix[idx] = embedding_vector\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10103, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Archtecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1007 02:26:31.418581 75684 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1007 02:26:31.471668 75684 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1007 02:26:31.484382 75684 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1007 02:26:31.500951 75684 deprecation.py:506] From C:\\Users\\asus\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1007 02:26:31.519527 75684 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# image model\n",
    "\n",
    "input_img_fearures = Input(shape=(2048,))\n",
    "inp_img1 = Dropout(0.2)(input_img_fearures)\n",
    "inp_img2 = Dense(256)(inp_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 02:26:32.035727 75684 deprecation.py:323] From C:\\Users\\asus\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# caption model\n",
    "input_captions = Input(shape=(max_len,))\n",
    "inp_cap1 = Embedding(input_dim=vocab_size,output_dim=50, mask_zero=True)(input_captions)\n",
    "inp_cap2 = Dropout(0.2)(inp_cap1)\n",
    "inp_cap3 = LSTM(256)(inp_cap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1 = add([inp_img2,inp_cap3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size,activation='softmax')(decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine model\n",
    "model = Model(inputs=[input_img_fearures,input_captions], outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 02:26:32.110849 75684 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 49, 50)       505150      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 49, 50)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          314368      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10103)        2596471     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,006,325\n",
      "Trainable params: 3,501,175\n",
      "Non-trainable params: 505,150\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pre init embedding layer\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 02:26:37.467180 75684 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compile Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_discriptions.txt\",\"w\") as f:\n",
    "    f.write(str(train_descriptions))\n",
    "with open(\"word_2_idx\",\"w\") as f:\n",
    "    f.write(str(word_2_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 4.3727\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1356s 344ms/step - loss: 3.7600\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 3.5856\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1357s 344ms/step - loss: 3.4976\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1357s 344ms/step - loss: 3.4439\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 3.4052\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1357s 344ms/step - loss: 3.3775\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 3.3599\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 345ms/step - loss: 3.3481\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 345ms/step - loss: 3.3379\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 3.3309\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 3.3271\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 345ms/step - loss: 3.3179\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3155\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 345ms/step - loss: 3.3111\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3115\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3079\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3083\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3044\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3137\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3060\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3093\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 345ms/step - loss: 3.3078\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3116\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3075\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3104\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3114\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3114\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 345ms/step - loss: 3.3175\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1359s 345ms/step - loss: 3.3176\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1371s 348ms/step - loss: 3.3158\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 3.3155\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 3.3165\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1364s 346ms/step - loss: 3.3176\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1364s 346ms/step - loss: 3.3264\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 344ms/step - loss: 3.3219\n",
      "Epoch 1/1\n",
      "3942/3942 [==============================] - 1358s 345ms/step - loss: 3.3209\n",
      "Epoch 1/1\n",
      "1464/3942 [==========>...................] - ETA: 14:22 - loss: 3.3227"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-f0e1e533082d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_2_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Data/model_weights/model_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mend_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Training of madel\n",
    "\n",
    "epochs = 120\n",
    "batch_size = 30\n",
    "steps = len(train_descriptions)//batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions,encoding_train,word_2_idx,max_len,batch_size)\n",
    "    model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n",
    "    model.save(('./Data/model_weights/model_'+str(i)+'.h5'))\n",
    "end_t = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_GPU",
   "language": "python",
   "name": "ml_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
