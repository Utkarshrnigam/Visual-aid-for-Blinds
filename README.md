# Introduction

- Without vision it can be challenging for a visually impaired person to examine the things happening in the surrounding. Even with aid, such as walking stick, it is not possible to get the idea of objects in the surrounding.
This project  makes the blind aware about the surroundings using video captioning and object detection. It is real time video captioning , which captures the frame from video and generates the caption for it and converts the generated caption into a voice using text to voice API.



##  :beginner: About
- Used ResNet50 model to convert the images to feature vectors.
- Used Glove embeddings to for the captions.
- Used Kesras functional api to create a custom input model for the training of the Neural network.


## :zap: Usage
- This project ca be used by Blinds.
_ This hepls the blind to locate the nearby objects with some surrounding context using english language. 

###  :electric_plug: Installation
- Clone the repository.
- Install all the depedencies.
- From the visual_aid for Blinds.ipynb notebook run the last cell.


### :notebook: Depedencies
List all the pre-requisites the system needs to develop this project.
- Tensorflow-GPU
- numpy
- pandas
- Re
- Keras
- pickle
- PIL
- Pyttsx3
- IPWebcam on both your android and PC device.
- SoundWire Server on both your android and PC device.


## ScreenShot

![](./CAPTURE.png)
